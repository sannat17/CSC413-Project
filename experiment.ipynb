{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid, RandomPartitionGraphDataset\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv, GATv2Conv, GATConv, SAGEConv, GINConv, TransformerConv\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Model and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "homophily_ratios = [round(0.1*x, 1) for x in range(1, 10)]\n",
    "models = {}\n",
    "epoch_train_lists = {}\n",
    "epoch_val_lists = {}\n",
    "epoch_test_lists = {}\n",
    "\n",
    "# hyperparameters\n",
    "args = {\"num_datasets_per_ratio\": 3,\n",
    "        \"num_nodes_per_class\": 400,\n",
    "        \"average_degree\": 10,\n",
    "        \"epochs\": 300,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"hidden_channels\": 128,\n",
    "        \"layer\": \"GCNConv\",  # GCNConv, SAGEConv, GATConv, GATv2Conv, TransformerConv\n",
    "        \"bias\": True, \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the cora dataset to get its features\n",
    "path_real = osp.join(\"/mnt/nas2/GNN-DATA/PYG\", Planetoid.__name__)\n",
    "dataset_real = Planetoid(path_real, \"Cora\")\n",
    "data_real = dataset_real[0]\n",
    "\n",
    "path_syn = osp.join(\"/mnt/nas2/GNN-DATA/PYG\", RandomPartitionGraphDataset.__name__)\n",
    "\n",
    "################################# Dataset generation ###########################\n",
    "\n",
    "\n",
    "def generate_datasets(K, average_degree, num_nodes_per_class, homophily_ratios):\n",
    "    datasets = {}\n",
    "\n",
    "    for node_homophily_ratio in homophily_ratios:\n",
    "        data_syn_list = []\n",
    "        for i in range(K):\n",
    "            dataset_syn = RandomPartitionGraphDataset(\n",
    "                path_syn,\n",
    "                num_channels=dataset_real.num_features,\n",
    "                num_classes=dataset_real.num_classes,\n",
    "                num_nodes_per_class=num_nodes_per_class,\n",
    "                node_homophily_ratio=node_homophily_ratio,\n",
    "                average_degree=average_degree,\n",
    "                # **kws,  # node_homophily_ratio=0.63, average_degree=3.90,\n",
    "                transform=T.Compose([\n",
    "                    T.RandomNodeSplit(\"random\", num_splits=K),\n",
    "                ]),\n",
    "            )\n",
    "            data_syn = dataset_syn[0]\n",
    "            data_syn_list.append(data_syn)\n",
    "        datasets[node_homophily_ratio] = data_syn_list\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = generate_datasets(args[\"num_datasets_per_ratio\"], args[\"average_degree\"], args[\"num_nodes_per_class\"], homophily_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, args, bias=True):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        hidden_channels=self.args[\"hidden_channels\"]\n",
    "        layer = self.args[\"layer\"]\n",
    "        _cls = eval(layer)\n",
    "        if layer == \"GCNConv\" or layer == \"SAGEConv\":\n",
    "            self.conv1 = _cls(dataset_real.num_features, hidden_channels, bias=bias)\n",
    "            self.conv2 = _cls(hidden_channels, dataset_real.num_classes)\n",
    "        elif layer == \"GATConv\" or layer == \"TransformerConv\" or layer == \"GATv2Conv\":\n",
    "            self.conv1 = _cls(dataset_real.num_features, hidden_channels // 8, heads=8, bias=bias)\n",
    "            self.conv2 = _cls(hidden_channels, dataset_real.num_classes, bias=bias)\n",
    "        else:\n",
    "            raise ValueError(f\"Wrong layer: {layer}\")\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.args[\"lr\"], weight_decay=self.args[\"weight_decay\"])\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        conv_layer_1 = self.conv1(x, edge_index, edge_attr)\n",
    "        relu_layer = F.elu(conv_layer_1)\n",
    "        drop_out_layer = F.dropout(relu_layer)\n",
    "        conv_layer_2 = self.conv2(drop_out_layer, edge_index, edge_attr)\n",
    "        return F.log_softmax(conv_layer_2, dim=1)\n",
    "    \n",
    "\n",
    "    def embed(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        return self.conv1(x, edge_index, edge_attr)\n",
    "\n",
    "    def train_model(self, data, idx=None):\n",
    "        self.train()\n",
    "\n",
    "        if data.train_mask.dim() >= 2 and idx is not None:\n",
    "            mask = data.train_mask[:, idx]\n",
    "        else:\n",
    "            mask = data.train_mask\n",
    "        (F.nll_loss(self(data)[mask], data.y[mask]) / self.args[\"num_datasets_per_ratio\"]).backward()\n",
    "\n",
    "        if idx is None or idx == self.args[\"num_datasets_per_ratio\"] - 1 or self.args[\"num_datasets_per_ratio\"] == 1:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, data, idx=None):\n",
    "        self.eval()\n",
    "        log_probs, accs = self(data), []\n",
    "        for _, mask in data(\"train_mask\", \"val_mask\", \"test_mask\"):\n",
    "\n",
    "            if mask.dim() >= 2 and idx is not None:\n",
    "                mask = mask[:, idx]\n",
    "\n",
    "            pred = log_probs[mask].max(1)[1]\n",
    "            acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "            accs.append(acc)\n",
    "        return accs\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def logistic_regression_test(self, data, h_type):\n",
    "        reg = LogisticRegression()\n",
    "        self.eval()\n",
    "        if h_type == \"x\":\n",
    "            h = data.x.cpu()\n",
    "        elif h_type == \"h1\":\n",
    "            h = self.embed(data).cpu()\n",
    "        elif h_type == \"h2\":\n",
    "            h = self(data_real).cpu()\n",
    "        else:\n",
    "            raise ValueError(f\"Wrong type: {h_type}\")\n",
    "\n",
    "        y = data.y.cpu()\n",
    "        reg.fit(h[data.train_mask].numpy(),\n",
    "                y[data.train_mask].numpy())\n",
    "        lr_test_acc = reg.score(h[data.test_mask].numpy(),\n",
    "                                y[data.test_mask].numpy())\n",
    "        return lr_test_acc\n",
    "\n",
    "    def fit(self, data_syn_list, verbose=True):\n",
    "        epoch_train_list, epoch_val_list, epoch_test_list = [], [], []\n",
    "\n",
    "        for epoch in range(self.args[\"epochs\"]):\n",
    "            random.shuffle(data_syn_list)\n",
    "\n",
    "            train_list, val_list, test_list = [], [], []\n",
    "            \n",
    "            for data_syn in data_syn_list:\n",
    "                self.train_model(data_syn, idx=epoch % self.args[\"num_datasets_per_ratio\"])\n",
    "\n",
    "                train_acc, val_acc, test_acc = self.test(data_syn, idx=epoch % self.args[\"num_datasets_per_ratio\"])\n",
    "                train_list.append(train_acc)\n",
    "                val_list.append(val_acc)\n",
    "                test_list.append(test_acc)\n",
    "\n",
    "            epoch_train_list.append(np.mean(train_list)) # per epoch, avg train acc\n",
    "            epoch_val_list.append(np.mean(val_list)) # per epoch, avg val acc\n",
    "            epoch_test_list.append(np.mean(test_list)) # per epoch, avg test acc\n",
    "\n",
    "            if epoch % 10 == 0 and verbose:\n",
    "                print(\n",
    "                    f\"{self.args['layer']} | \"\n",
    "                    f\"Epoch: {epoch:03d}, \"\n",
    "                    f\"Train: {np.mean(train_list):.4f}, \"\n",
    "                    f\"Val: {np.mean(val_list):.4f}, \"\n",
    "                    f\"Test: {np.mean(test_list):.4f}, \"\n",
    "                    f'LRT X: {self.logistic_regression_test(data_real, \"x\"):.4f}, '\n",
    "                    f'LRT H1: {self.logistic_regression_test(data_real, \"h1\"):.4f}, '\n",
    "                    f'LRT H2: {self.logistic_regression_test(data_real, \"h2\"):.4f}'\n",
    "                )\n",
    "\n",
    "        return epoch_train_list, epoch_val_list, epoch_test_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evalutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio in homophily_ratios:\n",
    "    print(\"======= Starting training for homophily =\", ratio, \"===============\")\n",
    "    model = Net(args)\n",
    "    epoch_train_list, epoch_val_list, epoch_test_list = model.fit(data_syn_list=datasets[ratio])\n",
    "    models[ratio] = model\n",
    "    epoch_train_lists[ratio], epoch_val_lists[ratio], epoch_test_lists[ratio] = epoch_train_list, epoch_val_list, epoch_test_list\n",
    "    print(\"======= Ended training for homophily =\", ratio, \"===============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio = 0.9\n",
    "# print(\"======= Starting training for homophily =\", ratio, \"===============\")\n",
    "# model = Net(args)\n",
    "# epoch_train_list, epoch_val_list, epoch_test_list = model.fit(data_syn_list=datasets[ratio])\n",
    "# models[ratio] = model\n",
    "# epoch_train_lists[ratio], epoch_val_lists[ratio], epoch_test_lists[ratio] = epoch_train_list, epoch_val_list, epoch_test_list\n",
    "# print(\"======= Ended training for homophily =\", ratio, \"===============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_300 = {\n",
    "    \"models\": models,\n",
    "    \"epoch_train_lists\": epoch_train_lists,\n",
    "    \"epoch_val_lists\": epoch_val_lists,\n",
    "    \"epoch_test_lists\": epoch_test_lists,\n",
    "    \"args\": args,\n",
    "    \"datasets\": datasets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./results/GCN_300_2022-04-24-10:50pm\", \"wb\") as outfile:\n",
    "    pickle.dump(GCN_300, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_plot(train_accuracy, validation_accuracy, homophily_ratio):\n",
    "    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(train_accuracy)), train_accuracy)\n",
    "    plt.plot(range(len(validation_accuracy)), validation_accuracy)\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./training_plots/h_{homophily_ratio}_epoch_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the plot of all validation accuracies of models made on different homophily ratios\n",
    "def plot_accuracies_by_homophilies(accuracies: dict, homophily_ratios: list, filename: str):\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  for ratio in homophily_ratios:\n",
    " \n",
    "    accuracy = accuracies[ratio]\n",
    "    plt.plot(range(len(accuracy)), accuracy, label=f\"h={ratio}\")\n",
    "    plt.legend()\n",
    "\n",
    "  plt.xlabel(\"Epochs\", fontsize=16)\n",
    "  plt.ylabel(\"Accuracy\", fontsize=16)\n",
    "  plt.xticks(fontsize=14)\n",
    "  plt.yticks(fontsize=14)\n",
    "  plt.legend()\n",
    "  # plt.tight_layout()\n",
    "  plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracies over epochs\n",
    "plot_accuracies_by_homophilies(epoch_train_lists, homophily_ratios, \"./plots/epoch_train_accuracies_GCN_300.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation accuracies over epochs\n",
    "plot_accuracies_by_homophilies(epoch_val_lists, homophily_ratios, \"./plots/epoch_val_accuracies_GCN_300.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting final accuracies by homophily ratio\n",
    "plt.figure()\n",
    "plt.plot(homophily_ratios, [epoch_train_lists[i][-1] for i in homophily_ratios], label=\"Training\")\n",
    "plt.plot(homophily_ratios, [epoch_val_lists[i][-1] for i in homophily_ratios], label=\"Validation\")\n",
    "plt.plot(homophily_ratios, [epoch_test_lists[i][-1] for i in homophily_ratios], label=\"Testing\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Homophily ratio\", fontsize=16)\n",
    "plt.ylabel(\"Final Accuracy\", fontsize=16)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./plots/final_accuracies.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2_embed(model, data):\n",
    "    x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "    conv_layer_1 = model.conv1(x, edge_index, edge_attr)\n",
    "    relu_layer = F.elu(conv_layer_1)\n",
    "    conv_layer_2 = model.conv2(relu_layer, edge_index, edge_attr)\n",
    "    return conv_layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_similarity(model, dataset, node_class=0, layer=1, args=args):\n",
    "\n",
    "    embeddings =  conv2_embed(model, dataset) if layer == 2 else model.embed(dataset)\n",
    "\n",
    "    avg_sim = 0\n",
    "    observations = 0\n",
    "    for i in range( args[\"num_nodes_per_class\"] * node_class, \n",
    "                    args[\"num_nodes_per_class\"] * (node_class + 1)\n",
    "                ):\n",
    "        for j in range( i + 1, \n",
    "                        args[\"num_nodes_per_class\"] * (node_class + 1)\n",
    "                    ):\n",
    "            \n",
    "            x1 = embeddings[i].detach().numpy()\n",
    "            x2 = embeddings[j].detach().numpy()\n",
    "\n",
    "            y1 = dataset.y.numpy()[i]\n",
    "            y2 = dataset.y.numpy()[j]\n",
    "\n",
    "            assert(y1 != y2)\n",
    "\n",
    "            avg_sim += np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
    "            observations += 1\n",
    "\n",
    "    avg_sim /= observations\n",
    "\n",
    "    return avg_sim\n",
    "\n",
    "avg_sims = []\n",
    "for ratio in homophily_ratios:\n",
    "\n",
    "    model = models[ratio]\n",
    "    dataset = datasets[ratio][0]\n",
    "    avg_sim = average_similarity(model, dataset, node_class=0, layer=1)\n",
    "\n",
    "    avg_sims.append(avg_sim)\n",
    "\n",
    "    print(f\"Ratio: {ratio} \\t Avg in class similarity: {avg_sim}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_similarity_across(model, dataset, per_class=10, layer=1, args=args):\n",
    "\n",
    "    embeddings =  conv2_embed(model, dataset) if layer == 2 else model.embed(dataset)\n",
    "\n",
    "    to_compare = []\n",
    "\n",
    "    for i in range(7):\n",
    "        base = args[\"num_nodes_per_class\"] * i\n",
    "        for pick in range(base, base + per_class):\n",
    "            to_compare.append(pick)\n",
    "\n",
    "    avg_sim = 0\n",
    "    observations = 0\n",
    "    for i in range( args[\"num_nodes_per_class\"] * 7):\n",
    "        for j in to_compare:\n",
    "\n",
    "            if i != j:\n",
    "            \n",
    "                x1 = embeddings[i].detach().numpy()\n",
    "                x2 = embeddings[j].detach().numpy()\n",
    "\n",
    "                avg_sim += np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
    "                observations += 1\n",
    "\n",
    "    avg_sim /= observations\n",
    "\n",
    "    return avg_sim\n",
    "\n",
    "avg_sims_across = []\n",
    "for ratio in homophily_ratios:\n",
    "\n",
    "    model = models[ratio]\n",
    "    dataset = datasets[ratio][0]\n",
    "    avg_sim = average_similarity(model, dataset, node_class=0, layer=1)\n",
    "\n",
    "    avg_sims_across.append(avg_sim)\n",
    "\n",
    "    print(f\"Ratio: {ratio} \\t Avg similarity across: {avg_sim}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some = datasets[0.1][0].x[10].numpy()\n",
    "avg_sim = 0\n",
    "for i in range(1, 400):\n",
    "    other = datasets[0.1][0].x[i].numpy()\n",
    "    temp = np.dot(some, other) / (np.linalg.norm(some) * np.linalg.norm(other))\n",
    "    avg_sim += temp\n",
    "\n",
    "\n",
    "avg_sim /= 2800\n",
    "\n",
    "avg_sim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(homophily_ratios, avg_sims, label=\"In-class\")\n",
    "plt.plot(homophily_ratios, avg_sims_across, label=\"Across dataset\")\n",
    "\n",
    "plt.xlabel(\"Homophily ratio\", fontsize=16)\n",
    "plt.ylabel(\"Average similarity\", fontsize=16)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./plots/average_similarities.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode can be \"train\", \"test\", or \"val\"\n",
    "def make_confusion(model, data_list, filename=\"./plots/confusion.png\", mode=\"test\"):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "\n",
    "    for data in data_list:\n",
    "        log_probs, preds, labels = model(data), [], []\n",
    "\n",
    "        for _, mask in data(\"train_mask\", \"val_mask\", \"test_mask\"):\n",
    "            mask = mask[:, 2]\n",
    "\n",
    "            pred = log_probs[mask].max(1)[1].numpy()\n",
    "            label = data.y[mask]\n",
    "\n",
    "            preds.append(pred)\n",
    "            labels.append(label)\n",
    "\n",
    "        train_pred, val_pred, test_pred = preds\n",
    "        train_label, val_label, test_label = labels\n",
    "\n",
    "        if mode ==  \"train\":\n",
    "            y_pred.extend(train_pred)\n",
    "            y_true.extend(train_label)\n",
    "\n",
    "        elif mode == \"val\":\n",
    "            y_pred.extend(val_pred)\n",
    "            y_true.extend(val_label)\n",
    "\n",
    "        else:\n",
    "            y_pred.extend(test_pred)\n",
    "            y_true.extend(test_label)\n",
    "\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "\n",
    "    df_cm = pd.DataFrame(cf/np.sum(cf) *10, index = [i for i in range(7)],\n",
    "                     columns = [i for i in range(7)])\n",
    "    plt.figure(figsize = (12,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion(models[0.9], datasets[0.9], mode=\"test\", filename=\"./plots/confusion_0.9.png\")\n",
    "# datasets[0.1][0].train_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading from pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./results/GCN_300_final\", \"rb\") as infile:\n",
    "    GCN_300 = pickle.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GCN_300[\"models\"]\n",
    "epoch_train_lists = GCN_300[\"epoch_train_lists\"]\n",
    "epoch_val_lists = GCN_300[\"epoch_val_lists\"]\n",
    "epoch_test_lists = GCN_300[\"epoch_test_lists\"]\n",
    "args = GCN_300[\"args\"]\n",
    "datasets = GCN_300[\"datasets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0.1][0].train_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0.1][0].val_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[0.1][0].test_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d17847e6150febd9ff3680a320717ef824d55df723c1690f1029d86d0f342c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('CSC413')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
